# User Guide

Complete guide for using the Evaluation feature in AI Dev Gallery.

## Getting Started

The Evaluation feature helps you assess AI model performance through three workflows:

### 1. Test Model
Generate and evaluate AI responses in real-time:
- Configure your model (API key, parameters)
- Upload a dataset of prompts
- Select evaluation metrics
- Run the evaluation

### 2. Evaluate Responses  
Score existing model outputs:
- Upload a dataset with pre-generated responses
- Configure evaluation criteria
- Apply metrics to assess quality
- View detailed results

### 3. Import Results
Bring in external evaluation data:
- Import JSONL files with evaluation results
- Supports various score formats (0-1 or 0-5 scale)
- Preserves individual item results
- Enables comparison with other evaluations

## Key Features

### Evaluation List
- **Multi-select** evaluations with checkboxes
- **Search** by name, model, or dataset
- **Status tracking** (Running, Completed, Failed, Imported)
- **Bulk operations** (compare, delete, export)

### Evaluation Insights
- **Score visualizations** with charts and graphs
- **Individual results browser** with file tree navigation
- **Statistical analysis** (mean, median, distribution)
- **Export options** (CSV, JSON, HTML reports)

### Comparison View
- Compare **2-5 evaluations** side-by-side
- **Model performance rankings**
- **Criteria-based analysis**
- **Visual difference highlighting**

## JSONL Import Format

Expected format for importing evaluation results:

```json
{"image_path": "images/cat.jpg", "prompt": "Describe this image", "response": "A cat sitting...", "scores": {"accuracy": 4.5, "relevance": 4.8}}
{"image_path": "images/dog.jpg", "prompt": "What do you see?", "response": "A dog playing...", "scores": {"accuracy": 4.2, "relevance": 4.6}}
```

Supported fields:
- `image_path` or `image` - Path to evaluated image
- `prompt` - Input prompt
- `response` - Model output
- `scores` or `criteria_scores` - Score dictionary
- `processing_time` - Time in seconds (optional)
- `error` - Error message if failed (optional)

## Tips & Best Practices

1. **Organize datasets** in folders for better analysis
2. **Use consistent metrics** across evaluations for comparison
3. **Name evaluations clearly** to identify them later
4. **Export important results** for backup or sharing
5. **Compare similar models** on the same dataset for best insights

## Troubleshooting

### Common Issues

**Import fails**
- Check JSONL format is valid JSON on each line
- Ensure required fields (scores) are present
- Verify file encoding is UTF-8

**Can't see evaluation details**
- Ensure evaluation status is "Completed" or "Imported"
- Check if evaluation has results data
- Try refreshing the evaluation list

**Comparison not available**
- Select 2-5 evaluations (no more, no less)
- Ensure evaluations have completed successfully
- Check that evaluations have score data

[‚Üê Back to Main Documentation](../README.md)
